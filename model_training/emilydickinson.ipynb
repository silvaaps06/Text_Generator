{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"/Users/AnaPSilva/Documents/Ana/Ironhack/Bootcamp/Final_Project/Data/Poem_Play/emilydickinson.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 69212 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I.                 \n",
      "LIFE.\n",
      "\n",
      "        I.\n",
      "\n",
      "I'm nobody!  Who are you?\n",
      "Are you nobody, too?\n",
      "Then there 's a pair of us -- don't tell!\n",
      "They 'd banish us, you know.\n",
      "\n",
      "How dreary to be somebody!\n",
      "How public, like a frog\n",
      "To tell your name the livelong day\n",
      "To an \n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Before training, you need to convert the strings to a numerical representation.\n",
    "## convert each character into a numeric ID. \n",
    "## It just needs the text to be split into tokens first.\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[39, 40, 41, 42, 43, 44, 45], [62, 63, 64]]>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## from tokens to character IDs\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## it will also be important to invert this representation and \n",
    "## recover human-readable strings from it.\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## join the characters back into strings.\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  \"\"\"join the characters back into strings\"\"\"\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(69212,), dtype=int64, numpy=array([22, 10,  2, ..., 46, 43,  3])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert the text vector into a stream of character indices.\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      ".\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'I' b'.' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b'\\n' b'L' b'I' b'F' b'E' b'.' b'\\n' b'\\n' b' '\n",
      " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b'I' b'.' b'\\n' b'\\n' b'I' b\"'\" b'm'\n",
      " b' ' b'n' b'o' b'b' b'o' b'd' b'y' b'!' b' ' b' ' b'W' b'h' b'o' b' '\n",
      " b'a' b'r' b'e' b' ' b'y' b'o' b'u' b'?' b'\\n' b'A' b'r' b'e' b' ' b'y'\n",
      " b'o' b'u' b' ' b'n' b'o' b'b' b'o' b'd' b'y' b',' b' ' b't' b'o' b'o'\n",
      " b'?' b'\\n' b'T' b'h' b'e' b'n' b' ' b't' b'h' b'e' b'r' b'e' b' ' b\"'\"\n",
      " b's' b' ' b'a'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "## The batch method lets you easily convert \n",
    "## these individual characters to sequences of the desired size.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"I.                 \\nLIFE.\\n\\n        I.\\n\\nI'm nobody!  Who are you?\\nAre you nobody, too?\\nThen there 's a\"\n",
      "b\" pair of us -- don't tell!\\nThey 'd banish us, you know.\\n\\nHow dreary to be somebody!\\nHow public, like \"\n",
      "b'a frog\\nTo tell your name the livelong day\\nTo an admiring bog!\\n\\n\\n\\n\\n        II.\\n\\nI bring an unaccustome'\n",
      "b'd wine\\nTo lips long parching, next to mine,\\nAnd summon them to drink.\\n\\nCrackling with fever, they ess'\n",
      "b'ay;\\nI turn my brimming eyes away,\\nAnd come next hour to look.\\n\\nThe hands still hug the tardy glass;\\nT'\n"
     ]
    }
   ],
   "source": [
    "## It's easier to see what this is doing if you join the \n",
    "## tokens back into strings\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    \"\"\"takes a sequence as input, duplicates, \n",
    "    and shifts it to align the input and label for each timestep\"\"\"\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b\"I.                 \\nLIFE.\\n\\n        I.\\n\\nI'm nobody!  Who are you?\\nAre you nobody, too?\\nThen there 's \"\n",
      "Target: b\".                 \\nLIFE.\\n\\n        I.\\n\\nI'm nobody!  Who are you?\\nAre you nobody, too?\\nThen there 's a\"\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Model\n",
    "\n",
    "- **tf.keras.layers.Embedding:** The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
    "- **tf.keras.layers.GRU:** A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
    "- **tf.keras.layers.Dense:** The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "display(vocab_size)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  16640     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62, 24,  2, 27, 56, 39, 30, 13, 33, 39, 24, 13, 57, 22, 56, 10, 36,\n",
       "       31, 51, 31,  1, 59, 24, 25, 45, 61, 29, 37, 13, 33, 13, 62, 48, 39,\n",
       "       13,  8, 19, 17, 36, 43, 20, 62, 30, 56, 48, 24, 56, 42, 36, 13, 44,\n",
       "       20, 25, 50, 42, 12, 23, 53, 35,  5,  8,  5, 16, 55,  1, 44, 22, 13,\n",
       "       49, 26,  4, 61, 20, 27,  3,  9, 29, 51, 52, 36, 19, 61, 55,  5,  7,\n",
       "       61, 30, 49,  5,  0, 25, 29, 20, 29, 15, 20,  8, 19, 23, 61])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "## This gives us, at each timestep, a prediction of the next character index\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'r dimples, too.\\n\\nI left the place with all my might, --\\nMy prayer away I threw;\\nThe quiet ages picke'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'xK NraQ?TaK?sIr.WRmR\\nuKLgwPX?T?xja?,FDWeGxQrjKrdW?fGLld;JoV\\',\\'Cq\\nfI?kM\"wGN!-PmnWFwq\\')wQk\\'[UNK]LPGPBG,FJw'\n"
     ]
    }
   ],
   "source": [
    "## Decode these to see the text predicted by this untrained model\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "- At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.17461, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.01449"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training procedure using the tf.keras.Model.compile method. \n",
    "## Use tf.keras.optimizers.Adam with default arguments and the loss function.\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_poem_emilydickinson_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 4.3930\n",
      "Epoch 2/225\n",
      "10/10 [==============================] - 53s 5s/step - loss: 3.8977\n",
      "Epoch 3/225\n",
      "10/10 [==============================] - 55s 5s/step - loss: 3.3164\n",
      "Epoch 4/225\n",
      "10/10 [==============================] - 55s 5s/step - loss: 3.1076\n",
      "Epoch 5/225\n",
      "10/10 [==============================] - 53s 5s/step - loss: 2.9612\n",
      "Epoch 6/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 2.8079\n",
      "Epoch 7/225\n",
      "10/10 [==============================] - 51s 5s/step - loss: 2.6690\n",
      "Epoch 8/225\n",
      "10/10 [==============================] - 50s 5s/step - loss: 2.5606\n",
      "Epoch 9/225\n",
      "10/10 [==============================] - 50s 5s/step - loss: 2.4725\n",
      "Epoch 10/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 2.4106\n",
      "Epoch 11/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 2.3504\n",
      "Epoch 12/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 2.3008\n",
      "Epoch 13/225\n",
      "10/10 [==============================] - 30s 3s/step - loss: 2.2621\n",
      "Epoch 14/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 2.2243\n",
      "Epoch 15/225\n",
      "10/10 [==============================] - 43s 4s/step - loss: 2.1936\n",
      "Epoch 16/225\n",
      "10/10 [==============================] - 46s 4s/step - loss: 2.1595\n",
      "Epoch 17/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 2.1279\n",
      "Epoch 18/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 2.1023\n",
      "Epoch 19/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 2.0747\n",
      "Epoch 20/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 2.0478\n",
      "Epoch 21/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 2.0202\n",
      "Epoch 22/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 1.9940\n",
      "Epoch 23/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.9715\n",
      "Epoch 24/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.9458\n",
      "Epoch 25/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 1.9238\n",
      "Epoch 26/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.8997\n",
      "Epoch 27/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 1.8746\n",
      "Epoch 28/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 1.8531\n",
      "Epoch 29/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.8345\n",
      "Epoch 30/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 1.8067\n",
      "Epoch 31/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 1.7884\n",
      "Epoch 32/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.7640\n",
      "Epoch 33/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.7438\n",
      "Epoch 34/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 1.7240\n",
      "Epoch 35/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 1.6950\n",
      "Epoch 36/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 1.6736\n",
      "Epoch 37/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 1.6472\n",
      "Epoch 38/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 1.6184\n",
      "Epoch 39/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 1.5923\n",
      "Epoch 40/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.5656\n",
      "Epoch 41/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.5354\n",
      "Epoch 42/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 1.5077\n",
      "Epoch 43/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 1.4765\n",
      "Epoch 44/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 1.4461\n",
      "Epoch 45/225\n",
      "10/10 [==============================] - 43s 4s/step - loss: 1.4109\n",
      "Epoch 46/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 1.3766\n",
      "Epoch 47/225\n",
      "10/10 [==============================] - 45s 4s/step - loss: 1.3379\n",
      "Epoch 48/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 1.2968\n",
      "Epoch 49/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 1.2551\n",
      "Epoch 50/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 1.2120\n",
      "Epoch 51/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 1.1684\n",
      "Epoch 52/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 1.1186\n",
      "Epoch 53/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.0678\n",
      "Epoch 54/225\n",
      "10/10 [==============================] - 45s 4s/step - loss: 1.0168\n",
      "Epoch 55/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.9636\n",
      "Epoch 56/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.9087\n",
      "Epoch 57/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.8564\n",
      "Epoch 58/225\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.7976\n",
      "Epoch 59/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.7378\n",
      "Epoch 60/225\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.6792\n",
      "Epoch 61/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.6207\n",
      "Epoch 62/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.5673\n",
      "Epoch 63/225\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.5142\n",
      "Epoch 64/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.4639\n",
      "Epoch 65/225\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.4189\n",
      "Epoch 66/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.3732\n",
      "Epoch 67/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.3330\n",
      "Epoch 68/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.2965\n",
      "Epoch 69/225\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.2669\n",
      "Epoch 70/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.2378\n",
      "Epoch 71/225\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2140\n",
      "Epoch 72/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.1932\n",
      "Epoch 73/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.1739\n",
      "Epoch 74/225\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.1599\n",
      "Epoch 75/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.1477\n",
      "Epoch 76/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.1366\n",
      "Epoch 77/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.1271\n",
      "Epoch 78/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.1189\n",
      "Epoch 79/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.1136\n",
      "Epoch 80/225\n",
      "10/10 [==============================] - 46s 4s/step - loss: 0.1073\n",
      "Epoch 81/225\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.1022\n",
      "Epoch 82/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0973\n",
      "Epoch 83/225\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.0925\n",
      "Epoch 84/225\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.0892\n",
      "Epoch 85/225\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.0856\n",
      "Epoch 86/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0833\n",
      "Epoch 87/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0816\n",
      "Epoch 88/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0783\n",
      "Epoch 89/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0756\n",
      "Epoch 90/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0737\n",
      "Epoch 91/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0727\n",
      "Epoch 92/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0709\n",
      "Epoch 93/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0691\n",
      "Epoch 94/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0681\n",
      "Epoch 95/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0674\n",
      "Epoch 96/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0649\n",
      "Epoch 97/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0628\n",
      "Epoch 98/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0611\n",
      "Epoch 99/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0600\n",
      "Epoch 100/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0584\n",
      "Epoch 101/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0579\n",
      "Epoch 102/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0568\n",
      "Epoch 103/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0561\n",
      "Epoch 104/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0553\n",
      "Epoch 105/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0542\n",
      "Epoch 106/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0539\n",
      "Epoch 107/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0527\n",
      "Epoch 108/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0523\n",
      "Epoch 109/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0519\n",
      "Epoch 110/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0515\n",
      "Epoch 111/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0512\n",
      "Epoch 112/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0506\n",
      "Epoch 113/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0500\n",
      "Epoch 114/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0494\n",
      "Epoch 115/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0490\n",
      "Epoch 116/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0490\n",
      "Epoch 117/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0483\n",
      "Epoch 118/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0474\n",
      "Epoch 119/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0476\n",
      "Epoch 120/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0473\n",
      "Epoch 121/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0467\n",
      "Epoch 122/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0467\n",
      "Epoch 123/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0464\n",
      "Epoch 124/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0467\n",
      "Epoch 125/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0466\n",
      "Epoch 126/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0466\n",
      "Epoch 127/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0466\n",
      "Epoch 128/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0460\n",
      "Epoch 129/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0458\n",
      "Epoch 130/225\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.0468\n",
      "Epoch 131/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0468\n",
      "Epoch 132/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0467\n",
      "Epoch 133/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0462\n",
      "Epoch 134/225\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.0465\n",
      "Epoch 135/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0465\n",
      "Epoch 136/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0478\n",
      "Epoch 137/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0493\n",
      "Epoch 138/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0491\n",
      "Epoch 139/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0495\n",
      "Epoch 140/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0510\n",
      "Epoch 141/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0505\n",
      "Epoch 142/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0499\n",
      "Epoch 143/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0509\n",
      "Epoch 144/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0503\n",
      "Epoch 145/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0491\n",
      "Epoch 146/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0473\n",
      "Epoch 147/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0463\n",
      "Epoch 148/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0447\n",
      "Epoch 149/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0437\n",
      "Epoch 150/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0428\n",
      "Epoch 151/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0423\n",
      "Epoch 152/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0417\n",
      "Epoch 153/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0416\n",
      "Epoch 154/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0414\n",
      "Epoch 155/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0410\n",
      "Epoch 156/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0408\n",
      "Epoch 157/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0407\n",
      "Epoch 158/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0407\n",
      "Epoch 159/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0404\n",
      "Epoch 160/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0404\n",
      "Epoch 161/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0403\n",
      "Epoch 162/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0402\n",
      "Epoch 163/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0401\n",
      "Epoch 164/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0406\n",
      "Epoch 165/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0400\n",
      "Epoch 166/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0402\n",
      "Epoch 167/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0399\n",
      "Epoch 168/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0398\n",
      "Epoch 169/225\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0399\n",
      "Epoch 170/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0397\n",
      "Epoch 171/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0398\n",
      "Epoch 172/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0397\n",
      "Epoch 173/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0397\n",
      "Epoch 174/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0397\n",
      "Epoch 175/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0395\n",
      "Epoch 176/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0395\n",
      "Epoch 177/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0394\n",
      "Epoch 178/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0396\n",
      "Epoch 179/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0393\n",
      "Epoch 180/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0393\n",
      "Epoch 181/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0394\n",
      "Epoch 182/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0389\n",
      "Epoch 183/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0390\n",
      "Epoch 184/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0392\n",
      "Epoch 185/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0388\n",
      "Epoch 186/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0392\n",
      "Epoch 187/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0390\n",
      "Epoch 188/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0389\n",
      "Epoch 189/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0388\n",
      "Epoch 190/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0389\n",
      "Epoch 191/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0388\n",
      "Epoch 192/225\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0389\n",
      "Epoch 193/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0388\n",
      "Epoch 194/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0386\n",
      "Epoch 195/225\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0390\n",
      "Epoch 196/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0387\n",
      "Epoch 197/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0388\n",
      "Epoch 198/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0386\n",
      "Epoch 199/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0386\n",
      "Epoch 200/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0385\n",
      "Epoch 201/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0386\n",
      "Epoch 202/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0391\n",
      "Epoch 203/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0383\n",
      "Epoch 204/225\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0387\n",
      "Epoch 205/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0391\n",
      "Epoch 206/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0385\n",
      "Epoch 207/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0390\n",
      "Epoch 208/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.0398\n",
      "Epoch 209/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0409\n",
      "Epoch 210/225\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.0429\n",
      "Epoch 211/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0458\n",
      "Epoch 212/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0531\n",
      "Epoch 213/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0740\n",
      "Epoch 214/225\n",
      "10/10 [==============================] - 37s 3s/step - loss: 0.2474\n",
      "Epoch 215/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.3731\n",
      "Epoch 216/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.2831\n",
      "Epoch 217/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.1783\n",
      "Epoch 218/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.1136\n",
      "Epoch 219/225\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0750\n",
      "Epoch 220/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0585\n",
      "Epoch 221/225\n",
      "10/10 [==============================] - 36s 3s/step - loss: 0.0488\n",
      "Epoch 222/225\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0449\n",
      "Epoch 223/225\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0424\n",
      "Epoch 224/225\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0417\n",
      "Epoch 225/225\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0408\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 225\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a livid claw.\n",
      "\n",
      "The birds put up the bars to nests,\n",
      "The cattle fled to barns;\n",
      "There came one drop of eventained\n",
      "In any out atain.\n",
      "\n",
      "I then I wook a deeace that died, -- their fister from the rask\n",
      "Of vauterful rosts\n",
      "      The hills just tell the others clause,\n",
      "And cleebed the ged his stare!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        XXII. \n",
      "\n",
      "     PRECODEN SEC.\n",
      "\n",
      "The room with thee itself I sure, --\n",
      "And what time the weaver sleeps\n",
      "   Who spun the breadths of blue!\n",
      "\n",
      "Write me how the hiles -- that Jesuon out.\n",
      "\n",
      "  \n",
      "            VII. \n",
      "\n",
      "       IN THE GHEDED.\n",
      "\n",
      "Be only of the bode!\n",
      "\n",
      "But I, gropp\n",
      "dis stirp it erstaming chair,\n",
      "\n",
      "Some, too frain sele alone,\n",
      "\"thing in the ones that Mie.\n",
      "\n",
      "The brave a sort mengle of the baye\n",
      "I hourd I 'm  'r the old horizonts to deam.\n",
      "\n",
      "The hauph we touch the smmeres pouth,\n",
      "She ere the heaven recore.\n",
      "\n",
      "Putites, night, will just agoning;\n",
      "And triund, and voried\n",
      "That you, so late, consider me,\n",
      "The sparrow of your\n",
      "Beautie lows away.\n",
      "\n",
      "Obr fore that die aster\n",
      "For every little knoll,\n",
      "Busy neetless, on fermocr,\n",
      "Th \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.414824962615967\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([' '])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x146c990d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 11:41:25.051861: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emily Dickinson/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emily Dickinson/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'Emily Dickinson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25e5be68547a581d8f01812af829697d501d092f95c6ea40be0663306c4e71b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fproj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

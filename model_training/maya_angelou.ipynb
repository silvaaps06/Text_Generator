{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"/Users/AnaPSilva/Documents/Ana/Ironhack/Bootcamp/Final_Project/Data/Poem_Play/maya_angelou.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 26102 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BLACK FAMILY PLEDGE\n",
      "\n",
      "BECAUSE we have forgotten our ancestors,\n",
      "our children no longer give us honor.\n",
      "\n",
      "BECAUSE we have lost the path our ancestors cleared\n",
      "kneeling in perilous undergrowth,\n",
      "our children cannot find their way.\n",
      "\n",
      "BECAUSE we have banish\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 12:40:07.058820: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Before training, you need to convert the strings to a numerical representation.\n",
    "## convert each character into a numeric ID. \n",
    "## It just needs the text to be split into tokens first.\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[38, 39, 40, 41, 42, 43, 44], [61, 62, 63]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## from tokens to character IDs\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## it will also be important to invert this representation and \n",
    "## recover human-readable strings from it.\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## join the characters back into strings.\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  \"\"\"join the characters back into strings\"\"\"\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(26102,), dtype=int64, numpy=array([32, 21, 18, ...,  1,  1,  1])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert the text vector into a stream of character indices.\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "H\n",
      "E\n",
      " \n",
      "B\n",
      "L\n",
      "A\n",
      "C\n",
      "K\n",
      " \n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'T' b'H' b'E' b' ' b'B' b'L' b'A' b'C' b'K' b' ' b'F' b'A' b'M' b'I'\n",
      " b'L' b'Y' b' ' b'P' b'L' b'E' b'D' b'G' b'E' b'\\n' b'\\n' b'B' b'E' b'C'\n",
      " b'A' b'U' b'S' b'E' b' ' b'w' b'e' b' ' b'h' b'a' b'v' b'e' b' ' b'f'\n",
      " b'o' b'r' b'g' b'o' b't' b't' b'e' b'n' b' ' b'o' b'u' b'r' b' ' b'a'\n",
      " b'n' b'c' b'e' b's' b't' b'o' b'r' b's' b',' b'\\n' b'o' b'u' b'r' b' '\n",
      " b'c' b'h' b'i' b'l' b'd' b'r' b'e' b'n' b' ' b'n' b'o' b' ' b'l' b'o'\n",
      " b'n' b'g' b'e' b'r' b' ' b'g' b'i' b'v' b'e' b' ' b'u' b's' b' ' b'h'\n",
      " b'o' b'n' b'o'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "## The batch method lets you easily convert \n",
    "## these individual characters to sequences of the desired size.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'THE BLACK FAMILY PLEDGE\\n\\nBECAUSE we have forgotten our ancestors,\\nour children no longer give us hono'\n",
      "b'r.\\n\\nBECAUSE we have lost the path our ancestors cleared\\nkneeling in perilous undergrowth,\\nour childre'\n",
      "b'n cannot find their way.\\n\\nBECAUSE we have banished the God of our ancestors,\\nour children cannot pray'\n",
      "b'.\\n\\nBECAUSE the old wails of our ancestors have faded beyond our hearing,\\nour children cannot hear us '\n",
      "b'crying.\\n\\nBECAUSE we have abandoned our wisdom of mothering and fathering,\\nour befuddled children give'\n",
      "b' birth to children\\nthey neither want nor understand.\\n\\nBECAUSE we have forgotten how to love, the adve'\n",
      "b'rsary is within our\\ngates, an holds us up to the mirror of the world shouting,\\n\"Regard the loveless\"\\n'\n",
      "b'\\nTherefore we pledge to bind ourselves to one another, to embrace our\\nlowliest, to keep company with '\n",
      "b'our loneliest, to educate our illiterate,\\nto feed our starving, to clothe our ragged, to do all good '\n",
      "b'things,\\nknowing that we are more than keepers of our brothers and sisters.\\n\\nWe ARE our brothers and s'\n"
     ]
    }
   ],
   "source": [
    "## It's easier to see what this is doing if you join the \n",
    "## tokens back into strings\n",
    "\n",
    "for seq in sequences.take(10):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    \"\"\"takes a sequence as input, duplicates, \n",
    "    and shifts it to align the input and label for each timestep\"\"\"\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'THE BLACK FAMILY PLEDGE\\n\\nBECAUSE we have forgotten our ancestors,\\nour children no longer give us hon'\n",
      "Target: b'HE BLACK FAMILY PLEDGE\\n\\nBECAUSE we have forgotten our ancestors,\\nour children no longer give us hono'\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Model\n",
    "\n",
    "- **tf.keras.layers.Embedding:** The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
    "- **tf.keras.layers.GRU:** A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
    "- **tf.keras.layers.Dense:** The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "display(vocab_size)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 64) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16384     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  65600     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,020,288\n",
      "Trainable params: 4,020,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 12, 52,  1, 17, 31, 22, 18, 46, 58, 29, 43, 21, 28, 21, 55, 12,\n",
       "       56, 27, 54, 56,  6,  7, 50,  7,  0,  0, 32, 25,  2, 41,  0, 42, 33,\n",
       "        0, 28, 55, 33, 14, 45, 52, 47, 20, 44, 39, 30, 43, 15,  9, 53, 29,\n",
       "       29, 46, 13, 28, 19,  5, 60, 30, 50,  1, 42, 31, 36, 15, 30, 13, 50,\n",
       "        9, 62, 17, 39, 44, 10, 50,  0, 48, 47, 15, 57, 56, 41, 54, 45, 39,\n",
       "       29, 38, 60, 32, 20, 22, 32, 43, 63, 33, 12, 15, 25, 14, 23])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "## This gives us, at each timestep, a prediction of the next character index\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b' Garden of Babylon\\nHanging as eternal beauty\\nIn our collective memory\\nNot the Grand Canyon\\nKindled i'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"':o\\nDSIEiuPfHOHr:sNqs),m,[UNK][UNK]TL d[UNK]eU[UNK]OrUAhojGgbRfB.pPPi?OF(wRm\\neSYBR?m.yDbg0m[UNK]kjBtsdqhbPawTGITfzU:BLAJ\"\n"
     ]
    }
   ],
   "source": [
    "## Decode these to see the text predicted by this untrained model\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "- At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 64)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1589293, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.00296"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training procedure using the tf.keras.Model.compile method. \n",
    "## Use tf.keras.optimizers.Adam with default arguments and the loss function.\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_poem_mayaangelou_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "4/4 [==============================] - 30s 6s/step - loss: 4.0554\n",
      "Epoch 2/350\n",
      "4/4 [==============================] - 26s 6s/step - loss: 4.2778\n",
      "Epoch 3/350\n",
      "4/4 [==============================] - 27s 7s/step - loss: 3.9083\n",
      "Epoch 4/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 3.7892\n",
      "Epoch 5/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 3.5075\n",
      "Epoch 6/350\n",
      "4/4 [==============================] - 29s 7s/step - loss: 3.2619\n",
      "Epoch 7/350\n",
      "4/4 [==============================] - 23s 6s/step - loss: 3.0989\n",
      "Epoch 8/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 3.0517\n",
      "Epoch 9/350\n",
      "4/4 [==============================] - 23s 6s/step - loss: 2.9703\n",
      "Epoch 10/350\n",
      "4/4 [==============================] - 22s 6s/step - loss: 2.9170\n",
      "Epoch 11/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 2.8366\n",
      "Epoch 12/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 2.7565\n",
      "Epoch 13/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 2.6857\n",
      "Epoch 14/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2.6190\n",
      "Epoch 15/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.5655\n",
      "Epoch 16/350\n",
      "4/4 [==============================] - 23s 5s/step - loss: 2.5250\n",
      "Epoch 17/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.4919\n",
      "Epoch 18/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 2.4613\n",
      "Epoch 19/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 2.4363\n",
      "Epoch 20/350\n",
      "4/4 [==============================] - 22s 5s/step - loss: 2.4144\n",
      "Epoch 21/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 2.3940\n",
      "Epoch 22/350\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.3757\n",
      "Epoch 23/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.3592\n",
      "Epoch 24/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.3423\n",
      "Epoch 25/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.3273\n",
      "Epoch 26/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 2.3116\n",
      "Epoch 27/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.2984\n",
      "Epoch 28/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.2812\n",
      "Epoch 29/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 2.2692\n",
      "Epoch 30/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 2.2538\n",
      "Epoch 31/350\n",
      "4/4 [==============================] - 22s 5s/step - loss: 2.2421\n",
      "Epoch 32/350\n",
      "4/4 [==============================] - 27s 7s/step - loss: 2.2278\n",
      "Epoch 33/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 2.2117\n",
      "Epoch 34/350\n",
      "4/4 [==============================] - 23s 6s/step - loss: 2.1994\n",
      "Epoch 35/350\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.1829\n",
      "Epoch 36/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 2.1705\n",
      "Epoch 37/350\n",
      "4/4 [==============================] - 32s 8s/step - loss: 2.1558\n",
      "Epoch 38/350\n",
      "4/4 [==============================] - 22s 5s/step - loss: 2.1401\n",
      "Epoch 39/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 2.1261\n",
      "Epoch 40/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 2.1093\n",
      "Epoch 41/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 2.0947\n",
      "Epoch 42/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 2.0763\n",
      "Epoch 43/350\n",
      "4/4 [==============================] - 25s 7s/step - loss: 2.0614\n",
      "Epoch 44/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 2.0428\n",
      "Epoch 45/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 2.0274\n",
      "Epoch 46/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 2.0110\n",
      "Epoch 47/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 1.9921\n",
      "Epoch 48/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.9744\n",
      "Epoch 49/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.9555\n",
      "Epoch 50/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9386\n",
      "Epoch 51/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.9185\n",
      "Epoch 52/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8979\n",
      "Epoch 53/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.8770\n",
      "Epoch 54/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.8558\n",
      "Epoch 55/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8346\n",
      "Epoch 56/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.8150\n",
      "Epoch 57/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7905\n",
      "Epoch 58/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7690\n",
      "Epoch 59/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.7450\n",
      "Epoch 60/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7192\n",
      "Epoch 61/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 1.6966\n",
      "Epoch 62/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 1.6710\n",
      "Epoch 63/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 1.6412\n",
      "Epoch 64/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 1.6154\n",
      "Epoch 65/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.5854\n",
      "Epoch 66/350\n",
      "4/4 [==============================] - 22s 5s/step - loss: 1.5556\n",
      "Epoch 67/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 1.5232\n",
      "Epoch 68/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.4942\n",
      "Epoch 69/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.4591\n",
      "Epoch 70/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.4257\n",
      "Epoch 71/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.3960\n",
      "Epoch 72/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.3534\n",
      "Epoch 73/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.3139\n",
      "Epoch 74/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 1.2744\n",
      "Epoch 75/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.2336\n",
      "Epoch 76/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.1903\n",
      "Epoch 77/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.1488\n",
      "Epoch 78/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 1.1019\n",
      "Epoch 79/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.0592\n",
      "Epoch 80/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 1.0126\n",
      "Epoch 81/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.9642\n",
      "Epoch 82/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.9115\n",
      "Epoch 83/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.8643\n",
      "Epoch 84/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.8165\n",
      "Epoch 85/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.7696\n",
      "Epoch 86/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.7180\n",
      "Epoch 87/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.6661\n",
      "Epoch 88/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.6222\n",
      "Epoch 89/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.5778\n",
      "Epoch 90/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.5347\n",
      "Epoch 91/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.4888\n",
      "Epoch 92/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.4484\n",
      "Epoch 93/350\n",
      "4/4 [==============================] - 22s 5s/step - loss: 0.4082\n",
      "Epoch 94/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.3749\n",
      "Epoch 95/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.3416\n",
      "Epoch 96/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.3069\n",
      "Epoch 97/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.2813\n",
      "Epoch 98/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.2552\n",
      "Epoch 99/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.2329\n",
      "Epoch 100/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.2131\n",
      "Epoch 101/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.1953\n",
      "Epoch 102/350\n",
      "4/4 [==============================] - 30s 7s/step - loss: 0.1795\n",
      "Epoch 103/350\n",
      "4/4 [==============================] - 34s 9s/step - loss: 0.1651\n",
      "Epoch 104/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.1533\n",
      "Epoch 105/350\n",
      "4/4 [==============================] - 25s 5s/step - loss: 0.1431\n",
      "Epoch 106/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 0.1335\n",
      "Epoch 107/350\n",
      "4/4 [==============================] - 23s 5s/step - loss: 0.1256\n",
      "Epoch 108/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.1182\n",
      "Epoch 109/350\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.1120\n",
      "Epoch 110/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.1063\n",
      "Epoch 111/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.1014\n",
      "Epoch 112/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0972\n",
      "Epoch 113/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0935\n",
      "Epoch 114/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0896\n",
      "Epoch 115/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0865\n",
      "Epoch 116/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0836\n",
      "Epoch 117/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0804\n",
      "Epoch 118/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0782\n",
      "Epoch 119/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0757\n",
      "Epoch 120/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0742\n",
      "Epoch 121/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0729\n",
      "Epoch 122/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0706\n",
      "Epoch 123/350\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.0687\n",
      "Epoch 124/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0669\n",
      "Epoch 125/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0653\n",
      "Epoch 126/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0640\n",
      "Epoch 127/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0625\n",
      "Epoch 128/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0612\n",
      "Epoch 129/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.0600\n",
      "Epoch 130/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0592\n",
      "Epoch 131/350\n",
      "4/4 [==============================] - 26s 6s/step - loss: 0.0578\n",
      "Epoch 132/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0569\n",
      "Epoch 133/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.0559\n",
      "Epoch 134/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0549\n",
      "Epoch 135/350\n",
      "4/4 [==============================] - 21s 6s/step - loss: 0.0539\n",
      "Epoch 136/350\n",
      "4/4 [==============================] - 24s 5s/step - loss: 0.0530\n",
      "Epoch 137/350\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.0522\n",
      "Epoch 138/350\n",
      "4/4 [==============================] - 26s 6s/step - loss: 0.0515\n",
      "Epoch 139/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0507\n",
      "Epoch 140/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0500\n",
      "Epoch 141/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0498\n",
      "Epoch 142/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 0.0490\n",
      "Epoch 143/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0487\n",
      "Epoch 144/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0480\n",
      "Epoch 145/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0476\n",
      "Epoch 146/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0469\n",
      "Epoch 147/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.0463\n",
      "Epoch 148/350\n",
      "4/4 [==============================] - 26s 6s/step - loss: 0.0458\n",
      "Epoch 149/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0452\n",
      "Epoch 150/350\n",
      "4/4 [==============================] - 23s 6s/step - loss: 0.0452\n",
      "Epoch 151/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0447\n",
      "Epoch 152/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.0441\n",
      "Epoch 153/350\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.0435\n",
      "Epoch 154/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0432\n",
      "Epoch 155/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0427\n",
      "Epoch 156/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0422\n",
      "Epoch 157/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0418\n",
      "Epoch 158/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0414\n",
      "Epoch 159/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0410\n",
      "Epoch 160/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0407\n",
      "Epoch 161/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0404\n",
      "Epoch 162/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0401\n",
      "Epoch 163/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0397\n",
      "Epoch 164/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0395\n",
      "Epoch 165/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0390\n",
      "Epoch 166/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0388\n",
      "Epoch 167/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0387\n",
      "Epoch 168/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0386\n",
      "Epoch 169/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0382\n",
      "Epoch 170/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0378\n",
      "Epoch 171/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0376\n",
      "Epoch 172/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0375\n",
      "Epoch 173/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0377\n",
      "Epoch 174/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0378\n",
      "Epoch 175/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0384\n",
      "Epoch 176/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0401\n",
      "Epoch 177/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0403\n",
      "Epoch 178/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0404\n",
      "Epoch 179/350\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.0409\n",
      "Epoch 180/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0399\n",
      "Epoch 181/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0388\n",
      "Epoch 182/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0393\n",
      "Epoch 183/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0381\n",
      "Epoch 184/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0378\n",
      "Epoch 185/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0379\n",
      "Epoch 186/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0368\n",
      "Epoch 187/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0363\n",
      "Epoch 188/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0355\n",
      "Epoch 189/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0353\n",
      "Epoch 190/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0348\n",
      "Epoch 191/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0347\n",
      "Epoch 192/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0345\n",
      "Epoch 193/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0342\n",
      "Epoch 194/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0339\n",
      "Epoch 195/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0337\n",
      "Epoch 196/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.0336\n",
      "Epoch 197/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0332\n",
      "Epoch 198/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0331\n",
      "Epoch 199/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0329\n",
      "Epoch 200/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0328\n",
      "Epoch 201/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0326\n",
      "Epoch 202/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0325\n",
      "Epoch 203/350\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.0325\n",
      "Epoch 204/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.0323\n",
      "Epoch 205/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0321\n",
      "Epoch 206/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0320\n",
      "Epoch 207/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0318\n",
      "Epoch 208/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.0322\n",
      "Epoch 209/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0322\n",
      "Epoch 210/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 0.0322\n",
      "Epoch 211/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.0320\n",
      "Epoch 212/350\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.0318\n",
      "Epoch 213/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0318\n",
      "Epoch 214/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0314\n",
      "Epoch 215/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0314\n",
      "Epoch 216/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0312\n",
      "Epoch 217/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0312\n",
      "Epoch 218/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0309\n",
      "Epoch 219/350\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.0309\n",
      "Epoch 220/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0309\n",
      "Epoch 221/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0308\n",
      "Epoch 222/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0308\n",
      "Epoch 223/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.0307\n",
      "Epoch 224/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0305\n",
      "Epoch 225/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0304\n",
      "Epoch 226/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0304\n",
      "Epoch 227/350\n",
      "4/4 [==============================] - 15s 3s/step - loss: 0.0303\n",
      "Epoch 228/350\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.0303\n",
      "Epoch 229/350\n",
      "4/4 [==============================] - 23s 5s/step - loss: 0.0303\n",
      "Epoch 230/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0307\n",
      "Epoch 231/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0304\n",
      "Epoch 232/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0309\n",
      "Epoch 233/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0308\n",
      "Epoch 234/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0314\n",
      "Epoch 235/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0357\n",
      "Epoch 236/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0390\n",
      "Epoch 237/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0378\n",
      "Epoch 238/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0403\n",
      "Epoch 239/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0399\n",
      "Epoch 240/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0406\n",
      "Epoch 241/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0413\n",
      "Epoch 242/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0418\n",
      "Epoch 243/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0413\n",
      "Epoch 244/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0385\n",
      "Epoch 245/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0373\n",
      "Epoch 246/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0352\n",
      "Epoch 247/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0339\n",
      "Epoch 248/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0337\n",
      "Epoch 249/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0320\n",
      "Epoch 250/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0315\n",
      "Epoch 251/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0310\n",
      "Epoch 252/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0306\n",
      "Epoch 253/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 0.0303\n",
      "Epoch 254/350\n",
      "4/4 [==============================] - 23s 5s/step - loss: 0.0301\n",
      "Epoch 255/350\n",
      "4/4 [==============================] - 24s 6s/step - loss: 0.0298\n",
      "Epoch 256/350\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.0298\n",
      "Epoch 257/350\n",
      "4/4 [==============================] - 26s 6s/step - loss: 0.0295\n",
      "Epoch 258/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0294\n",
      "Epoch 259/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0293\n",
      "Epoch 260/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0293\n",
      "Epoch 261/350\n",
      "4/4 [==============================] - 23s 6s/step - loss: 0.0292\n",
      "Epoch 262/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0291\n",
      "Epoch 263/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0290\n",
      "Epoch 264/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0289\n",
      "Epoch 265/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0289\n",
      "Epoch 266/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0287\n",
      "Epoch 267/350\n",
      "4/4 [==============================] - 16s 3s/step - loss: 0.0288\n",
      "Epoch 268/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0286\n",
      "Epoch 269/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0285\n",
      "Epoch 270/350\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0284\n",
      "Epoch 271/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.0284\n",
      "Epoch 272/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0285\n",
      "Epoch 273/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0283\n",
      "Epoch 274/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0282\n",
      "Epoch 275/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0281\n",
      "Epoch 276/350\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0282\n",
      "Epoch 277/350\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0283\n",
      "Epoch 278/350\n",
      "4/4 [==============================] - 13s 4s/step - loss: 0.0282\n",
      "Epoch 279/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0281\n",
      "Epoch 280/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0280\n",
      "Epoch 281/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0279\n",
      "Epoch 282/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0279\n",
      "Epoch 283/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0279\n",
      "Epoch 284/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0278\n",
      "Epoch 285/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0277\n",
      "Epoch 286/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0277\n",
      "Epoch 287/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0277\n",
      "Epoch 288/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0276\n",
      "Epoch 289/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.0276\n",
      "Epoch 290/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0277\n",
      "Epoch 291/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0277\n",
      "Epoch 292/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0275\n",
      "Epoch 293/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0275\n",
      "Epoch 294/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0275\n",
      "Epoch 295/350\n",
      "4/4 [==============================] - 15s 3s/step - loss: 0.0275\n",
      "Epoch 296/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0275\n",
      "Epoch 297/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0276\n",
      "Epoch 298/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0275\n",
      "Epoch 299/350\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.0278\n",
      "Epoch 300/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0298\n",
      "Epoch 301/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0282\n",
      "Epoch 302/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0294\n",
      "Epoch 303/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0289\n",
      "Epoch 304/350\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0284\n",
      "Epoch 305/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0286\n",
      "Epoch 306/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0284\n",
      "Epoch 307/350\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0279\n",
      "Epoch 308/350\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0282\n",
      "Epoch 309/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0279\n",
      "Epoch 310/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0280\n",
      "Epoch 311/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0277\n",
      "Epoch 312/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0275\n",
      "Epoch 313/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0277\n",
      "Epoch 314/350\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0275\n",
      "Epoch 315/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0274\n",
      "Epoch 316/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0274\n",
      "Epoch 317/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0274\n",
      "Epoch 318/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0273\n",
      "Epoch 319/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0271\n",
      "Epoch 320/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0272\n",
      "Epoch 321/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0271\n",
      "Epoch 322/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0271\n",
      "Epoch 323/350\n",
      "4/4 [==============================] - 19s 4s/step - loss: 0.0271\n",
      "Epoch 324/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0270\n",
      "Epoch 325/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0270\n",
      "Epoch 326/350\n",
      "4/4 [==============================] - 15s 3s/step - loss: 0.0270\n",
      "Epoch 327/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0269\n",
      "Epoch 328/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0269\n",
      "Epoch 329/350\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0270\n",
      "Epoch 330/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0269\n",
      "Epoch 331/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0270\n",
      "Epoch 332/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0270\n",
      "Epoch 333/350\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0270\n",
      "Epoch 334/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0269\n",
      "Epoch 335/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0269\n",
      "Epoch 336/350\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0268\n",
      "Epoch 337/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0268\n",
      "Epoch 338/350\n",
      "4/4 [==============================] - 23s 6s/step - loss: 0.0268\n",
      "Epoch 339/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0268\n",
      "Epoch 340/350\n",
      "4/4 [==============================] - 20s 4s/step - loss: 0.0268\n",
      "Epoch 341/350\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0268\n",
      "Epoch 342/350\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0267\n",
      "Epoch 343/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0267\n",
      "Epoch 344/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0268\n",
      "Epoch 345/350\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0267\n",
      "Epoch 346/350\n",
      "4/4 [==============================] - 22s 5s/step - loss: 0.0267\n",
      "Epoch 347/350\n",
      "4/4 [==============================] - 21s 6s/step - loss: 0.0267\n",
      "Epoch 348/350\n",
      "4/4 [==============================] - 26s 6s/step - loss: 0.0267\n",
      "Epoch 349/350\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0267\n",
      "Epoch 350/350\n",
      "4/4 [==============================] - 23s 6s/step - loss: 0.0266\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 350\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life ppanos\n",
      "upon the laughte is muth and ages.\n",
      "But today, the Rock cries out to us, clearler she pity we have lived through and live through still,\n",
      "Have sharpened our senses and tough.\n",
      "She sent them away,\n",
      "underground, overland, in coaches and\n",
      "shoeless.\n",
      "\n",
      "When you learn, teachers she crick co sace ever in the gings of hearived and the shenes\n",
      "\n",
      "When we come to it\n",
      "We, this people, on this wayward, floating body\n",
      "Created on this earth, ofreg noedgnigg, the nears,\n",
      "The nearh of my haust,\n",
      "by lough you a bodder \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.1359128952026367\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Life '])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(500):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x14d306e50>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x14d306e50>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Maya Angelou/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Maya Angelou/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'Maya Angelou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25e5be68547a581d8f01812af829697d501d092f95c6ea40be0663306c4e71b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fproj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

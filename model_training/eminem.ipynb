{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"/Users/AnaPSilva/Documents/Ana/Ironhack/Bootcamp/Final_Project/Data/Rap/eminem.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 926405 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look, I was gonna go easy on you not to hurt your feelings\n",
      "But I'm only going to get this one chance\n",
      "(Six minutes, six minutes)\n",
      "Something's wrong, I can feel it\n",
      "(Six minutes, six minutes, Slim Shady, you're on)\n",
      "Just a feeling I've got\n",
      "Like something'\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 03:01:19.243618: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Before training, you need to convert the strings to a numerical representation.\n",
    "## convert each character into a numeric ID. \n",
    "## It just needs the text to be split into tokens first.\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[60, 61, 62, 63, 64, 65, 66], [83, 84, 85]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## from tokens to character IDs\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## it will also be important to invert this representation and \n",
    "## recover human-readable strings from it.\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## join the characters back into strings.\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  \"\"\"join the characters back into strings\"\"\"\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(926405,), dtype=int64, numpy=array([42, 74, 74, ..., 60, 72, 64])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert the text vector into a stream of character indices.\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "o\n",
      "o\n",
      "k\n",
      ",\n",
      " \n",
      "I\n",
      " \n",
      "w\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9172"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'L' b'o' b'o' b'k' b',' b' ' b'I' b' ' b'w' b'a' b's' b' ' b'g' b'o'\n",
      " b'n' b'n' b'a' b' ' b'g' b'o' b' ' b'e' b'a' b's' b'y' b' ' b'o' b'n'\n",
      " b' ' b'y' b'o' b'u' b' ' b'n' b'o' b't' b' ' b't' b'o' b' ' b'h' b'u'\n",
      " b'r' b't' b' ' b'y' b'o' b'u' b'r' b' ' b'f' b'e' b'e' b'l' b'i' b'n'\n",
      " b'g' b's' b'\\n' b'B' b'u' b't' b' ' b'I' b\"'\" b'm' b' ' b'o' b'n' b'l'\n",
      " b'y' b' ' b'g' b'o' b'i' b'n' b'g' b' ' b't' b'o' b' ' b'g' b'e' b't'\n",
      " b' ' b't' b'h' b'i' b's' b' ' b'o' b'n' b'e' b' ' b'c' b'h' b'a' b'n'\n",
      " b'c' b'e' b'\\n'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "## The batch method lets you easily convert \n",
    "## these individual characters to sequences of the desired size.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Look, I was gonna go easy on you not to hurt your feelings\\nBut I'm only going to get this one chance\\n\"\n",
      "b\"(Six minutes, six minutes)\\nSomething's wrong, I can feel it\\n(Six minutes, six minutes, Slim Shady, yo\"\n",
      "b\"u're on)\\nJust a feeling I've got\\nLike something's about to happen\\nBut I don't know what\\nIf that means\"\n",
      "b\", what I think it means, we're in trouble\\nBig trouble. And if he is as bananas as you say\\nI'm not tak\"\n",
      "b\"ing any chances\\nYou are just what the doc ordered\\n\\nI'm beginning to feel like a Rap God, Rap God\\nAll \"\n"
     ]
    }
   ],
   "source": [
    "## It's easier to see what this is doing if you join the \n",
    "## tokens back into strings\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    \"\"\"takes a sequence as input, duplicates, \n",
    "    and shifts it to align the input and label for each timestep\"\"\"\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b\"Look, I was gonna go easy on you not to hurt your feelings\\nBut I'm only going to get this one chance\"\n",
      "Target: b\"ook, I was gonna go easy on you not to hurt your feelings\\nBut I'm only going to get this one chance\\n\"\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Model\n",
    "\n",
    "- **tf.keras.layers.Embedding:** The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
    "- **tf.keras.layers.GRU:** A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
    "- **tf.keras.layers.Dense:** The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "display(vocab_size)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Making sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 111) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  28416     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  113775    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,080,495\n",
      "Trainable params: 4,080,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40,  89,  91,  42,  93,  17,  76,  89,  23, 103,  68,   9,  84,\n",
       "       104,  44,  71,  51,  83,  54, 107,  30,  99,   7,  72,  38,  11,\n",
       "        17,  82,  17,  98,   8, 109,  96,   8,  67,  68, 100,  65,  44,\n",
       "        79,  24,  60,  79,  47,  90,  93,  43,   1,  17,  60,  91,  22,\n",
       "         4,   2,  17,  20,  98,  78,  94,  16,  94,  93,  20,   4,  98,\n",
       "         6,  60,  53,  50,  80,  17, 101,  42,  15,  99,  56,  36,  44,\n",
       "         6,  10,  88,  31,  52,   3,  29,   3,  90,  76,  62,  82,  74,\n",
       "        38, 101,  86,   4,  55,  88, 108,   1,  93])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "## This gives us, at each timestep, a prediction of the next character index\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'\\nA big Chinese nigga, screamin \"Kuniva yo yo..\"\\nI leave ya face leakin, run up in church \\nand smack '\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"J\\xc3\\x85\\xc3\\xa0L\\xc3\\xa2/q\\xc3\\x855\\xe2\\x80\\x93i'y\\xe2\\x80\\x94NlUxX\\xe2\\x80\\x9c?\\xc3\\xb6%mH)/w/\\xc3\\xb3&\\xe2\\x80\\xa6\\xc3\\xa9&hi\\xc3\\xbafNt6atQ\\xc3\\x9f\\xc3\\xa2M\\n/a\\xc3\\xa04!\\r/2\\xc3\\xb3s\\xc3\\xa4.\\xc3\\xa4\\xc3\\xa22!\\xc3\\xb3$aWTu/\\xc3\\xbcL-\\xc3\\xb6ZFN$(\\xc2\\xa9AV ; \\xc3\\x9fqcwoH\\xc3\\xbc{!Y\\xc2\\xa9\\xe2\\x80\\x9d\\n\\xc3\\xa2\"\n"
     ]
    }
   ],
   "source": [
    "## Decode these to see the text predicted by this untrained model\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "- At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 111)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.709812, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.0313"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training procedure using the tf.keras.Model.compile method. \n",
    "## Use tf.keras.optimizers.Adam with default arguments and the loss function.\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_rap_eminem_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "143/143 [==============================] - 473s 3s/step - loss: 2.9159\n",
      "Epoch 2/35\n",
      "143/143 [==============================] - 434s 3s/step - loss: 2.1703\n",
      "Epoch 3/35\n",
      "143/143 [==============================] - 431s 3s/step - loss: 1.9063\n",
      "Epoch 4/35\n",
      "143/143 [==============================] - 434s 3s/step - loss: 1.7402\n",
      "Epoch 5/35\n",
      "143/143 [==============================] - 434s 3s/step - loss: 1.6219\n",
      "Epoch 6/35\n",
      "143/143 [==============================] - 434s 3s/step - loss: 1.5311\n",
      "Epoch 7/35\n",
      "143/143 [==============================] - 434s 3s/step - loss: 1.4554\n",
      "Epoch 8/35\n",
      "143/143 [==============================] - 434s 3s/step - loss: 1.3878\n",
      "Epoch 9/35\n",
      "143/143 [==============================] - 435s 3s/step - loss: 1.3238\n",
      "Epoch 10/35\n",
      "143/143 [==============================] - 437s 3s/step - loss: 1.2600\n",
      "Epoch 11/35\n",
      "143/143 [==============================] - 437s 3s/step - loss: 1.1944\n",
      "Epoch 12/35\n",
      "143/143 [==============================] - 443s 3s/step - loss: 1.1288\n",
      "Epoch 13/35\n",
      "143/143 [==============================] - 434s 3s/step - loss: 1.0598\n",
      "Epoch 14/35\n",
      "143/143 [==============================] - 427s 3s/step - loss: 0.9873\n",
      "Epoch 15/35\n",
      "143/143 [==============================] - 438s 3s/step - loss: 0.9136\n",
      "Epoch 16/35\n",
      "143/143 [==============================] - 431s 3s/step - loss: 0.8406\n",
      "Epoch 17/35\n",
      "143/143 [==============================] - 441s 3s/step - loss: 0.7688\n",
      "Epoch 18/35\n",
      "143/143 [==============================] - 431s 3s/step - loss: 0.7016\n",
      "Epoch 19/35\n",
      "143/143 [==============================] - 440s 3s/step - loss: 0.6408\n",
      "Epoch 20/35\n",
      "143/143 [==============================] - 430s 3s/step - loss: 0.5888\n",
      "Epoch 21/35\n",
      "143/143 [==============================] - 442s 3s/step - loss: 0.5429\n",
      "Epoch 22/35\n",
      "143/143 [==============================] - 428s 3s/step - loss: 0.5068\n",
      "Epoch 23/35\n",
      "143/143 [==============================] - 443s 3s/step - loss: 0.4753\n",
      "Epoch 24/35\n",
      "143/143 [==============================] - 427s 3s/step - loss: 0.4478\n",
      "Epoch 25/35\n",
      "143/143 [==============================] - 445s 3s/step - loss: 0.4252\n",
      "Epoch 26/35\n",
      "143/143 [==============================] - 428s 3s/step - loss: 0.4102\n",
      "Epoch 27/35\n",
      "143/143 [==============================] - 430s 3s/step - loss: 0.3993\n",
      "Epoch 28/35\n",
      "143/143 [==============================] - 426s 3s/step - loss: 0.3858\n",
      "Epoch 29/35\n",
      "143/143 [==============================] - 441s 3s/step - loss: 0.3776\n",
      "Epoch 30/35\n",
      "143/143 [==============================] - 417s 3s/step - loss: 0.3677\n",
      "Epoch 31/35\n",
      "143/143 [==============================] - 412s 3s/step - loss: 0.3566\n",
      "Epoch 32/35\n",
      "143/143 [==============================] - 411s 3s/step - loss: 0.3532\n",
      "Epoch 33/35\n",
      "143/143 [==============================] - 413s 3s/step - loss: 0.3454\n",
      "Epoch 34/35\n",
      "143/143 [==============================] - 412s 3s/step - loss: 0.3433\n",
      "Epoch 35/35\n",
      "143/143 [==============================] - 413s 3s/step - loss: 0.3408\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 35\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am just like her\n",
      "Everything I do wan disfed the world off\n",
      "\n",
      "The way you move it, you know what can I do?\n",
      "Mo passionates at a bride colired\n",
      "So I won't be sator, sew, but it's stolen\n",
      "Tryna pulk packetta what my exact turks and saw\n",
      "But I've never seen down the wind the clock\n",
      "I just got my onf my weenie is much before I know that you should\n",
      "Long as you know you're unsour\n",
      "No more piece of the indumonaly\n",
      "Million dollars ain't it hits around and do attention\n",
      "I'm just watching to conscience\n",
      "I'm turning back to the road, even though me and Doc hair hed man\n",
      "If at all the flame guess white ballin'\n",
      "They're hanger intertroned\n",
      "That though for murdering has married to explain\n",
      "I ain't murdered nobody\n",
      "I stand there and wait a motherfuckin' table\n",
      "We're only life it\n",
      "Got another lawndom\n",
      "But if you better afraid on the door\n",
      "Somebody's life, like a swear-to-carribreat, \n",
      "I'm Slim Shady, a thousand beat\n",
      "\"But she said \"No, I'm here\n",
      "And I ain't just playin and nails your cheek? Me?\n",
      "My eye 'em down, down (Wanowa d \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.519538164138794\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['a'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x12c538940>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x12c538940>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Eminem/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Eminem/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'Eminem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25e5be68547a581d8f01812af829697d501d092f95c6ea40be0663306c4e71b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fproj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
